For a complete and detailed implementation of ML using Snowpark Pyhton, see : https://github.com/Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python.git or https://towardsdatascience.com/machine-learning-on-snowflake-ea6c559af2d


Global architecture : 
    - For each language : 
        o Raw data table(s) ---> clean data ---> extract important data ---> pass into the model ---> save new translated data table
    - Translated Data table ---> pass into the model ---> save objectives relevance in a new table
    
How to implement and use a ML model in Snowflake using python : 
    - There's no model yet :
        o Create a train_model function, register the model training Python function as a Snowpark Python Stored Procedure using Session.sproc.register() : (1) or (2)
        o Train the model using Session.call() : (3)
    - There's already a trained model existing :
        o Create and register a Snowflake Python UDF and add the trained model as a dependency (the udf import packages and sets everything. To directly evaluate the model, use model.predict()) : (4)
        o To evaluate the model on a dataframe, use <the_data_frame>.call_udf() : (5)
    - The above evaluation is a scalar one, only one row is evaluated at once, to have less transformation logic required and to have better performance, use a vectorized UDF that takes as input Pandas DataFrames and return batches of results as Pandas arrays :
        o Create a new vectorized UDF using batch API for inference that will load the model once using cachetools (6) and then do a batch_predict udf : (7)
        o Call the vectorized UDF the same as with the scalar one : (5)


Ressources :
(1) https://interworks.com/blog/2022/09/20/a-definitive-guide-to-creating-python-stored-procedures-in-snowflake-using-snowpark/
(2) https://medium.com/snowflake/operationalizing-your-code-with-snowpark-python-stored-procedures-a16c77afcff
(3) https://docs.snowflake.com/en/developer-guide/snowpark/python/calling-functions
(4) https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udfs
(5) https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.call_udf.html
(6) The file is only read when the udf is created and won't be read when the udf is called, this is made possible with cachetools library : https://docs.snowflake.com/fr/developer-guide/snowpark/python/creating-udfs
(7) https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-batch#getting-started-with-the-batch-api

Useful documentation to implement all the above directly on a Snowsight worksheet : https://docs.snowflake.com/en/developer-guide/snowpark/python/index
